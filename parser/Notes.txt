Техзадание:
Написать Python-скрипт с использованием библиотеки Beautiful Soup по сбору данных с сайта www.reddit.com
по постам в категории Top -> This Month . Результат построчно (одна строка = один пост) сложить в текстовый файл
с именем reddit-YYYYMMDDHHMM.txt в следующем формате:

UNIQUE_ID;post URL;username;user karma;user cake day;post karma;comment karma;post date;number of comments;
number of votes;post category

UNIQUE_ID - буквенно-цифровой уникальный цифровой идентификатор записи длиной 32 символа, формируется при помощи
функции uuid1() библиотеки uuid с параметром hex.

Содержимое выходного файла должно формироваться на момент запуска программы за один раз. В имени выходного файла:
YYYY - год, например, 2020; MM - месяц, 12; DD - день; HH - часы; mm - минуты. В случае наличия файла
reddit-YYYYMMDDHHmm.txt , содержимое существующего файла удаляется, файл формируется заново.

Выходной файл должен содержать 100 записей заданного формата. В случае, если по причине ограничений reddit.com по
конкретному посту не удается собрать данные в нужном формате и полном объеме - пост игнорируется.

Скрипт должен логгировать все значимые события с использованием стандартной библиотеки logging.

Возможные ограничения reddit.com:

1) пост размещен, а пользователь, разместивший его, удален (пользователя больше не существует)
2) контент страницы недоступен без подтверждения возраста

https://www.reddit.com/top/?t=month

Количество лайков/поднятий
<div class="_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ" style="color: rgb(215, 218, 220);">259k</div>


 1 class="rpBJOHq2PR60pnwJlUyP0"
 Записи class="_1oQyIsiPHYt6nx7VOmd1sz _1RYN-7H8gYctjOQeL8p2Q7 scrollerItem _3Qkp11fjcAw9I9wtLo8frE _1qftyZQ2bhqP62lbPjoGAh  Post t3_qkq3a2 "

    items_divs = soup.find_all("div", class_="y8HYJ-y_lTUHkQIc1mdCq _2INHSNB8V5eaWp4P0rY_mE")

    urls = []
    for item in items_divs:
        item_url = item.find("div", class_="_2FCtq-QzlfuN-SwVMUZMM3 _3wiKjmhpIpoTE2r5KCm2o6 t3_q82vqk").\
            find("div", class_="y8HYJ-y_lTUHkQIc1mdCq _2INHSNB8V5eaWp4P0rY_mE").find("a").get("href")
        urls.append(item_url)

    with open("parser/data.txt", "w") as file:
        for url in urls:
            file.write(f"{url}\n")